# -*- mode:org -*-

please create a virtual environment and run

: pip install -r requirements.txt

See https://uoa-eresearch.github.io/eresearch-cookbook/recipe/2014/11/26/python-virtual-env/

* touch.py

prepare sentences for input to =sensetion.el= and elastic
search. provide one or more files with one sentence per line.

#+BEGIN_SRC sh
python3 touch.py --help
#+END_SRC

you'll need to have a configuration file for the REPP tokenizer; there
is one at =pet/repp.set= at http://svn.delph-in.net/erg/trunk.

this step only performs tokenization; you should run the =enrich.py=
script on its output too.

* enrich.py

the corpus prepared by =touch.py= is not sufficient by itself. this
script adds lemmatization and automatic sense-tagging (for unambiguous
words), and may do more in the future.

you'll need to [[https://www.nltk.org/data.html][download wordnet data from NLTK]] in order to run this
script.

#+begin_src python
import nltk
nltk.download('wordnet')
#+end_src

* es-dump.py

dump all documents in a given elasticsearch server to a chosen directory.

#+BEGIN_SRC sh
python3 es-dump.py --help
#+END_SRC
